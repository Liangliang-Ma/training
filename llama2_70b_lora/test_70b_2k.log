[2024-02-28 23:44:25,143] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
could not open any host key
ssh_keysign: no reply
sign using hostkey ssh-ed25519 SHA256:A2W4lNPcOoKo7omPDL4wGkCaq4d9EEhM1BwN4wf2ZTM failed
[2024-02-28 23:44:26,499] [INFO] [runner.py:463:main] Using IP address of 172.16.1.23 for node compute23
[2024-02-28 23:44:26,500] [INFO] [runner.py:568:main] cmd = mpirun -n 8 -ppn 8 -hostfile hostfile_mpich -genv PYTHONSTARTUP=/etc/pythonstart -genv PYTHONPATH=/home/maliangl/ft/reference/llama2_70b_lora -genv MASTER_ADDR 172.16.1.23 -genv MASTER_PORT 29500 -genv WORLD_SIZE 8 -genv LOCAL_SIZE 8 -hosts compute23 /home/maliangl/miniconda3/envs/ft/bin/python -u /home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/deepspeed/launcher/launcher_helper.py --launcher mpich scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 6 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
My guessed rank = 5
My guessed rank = 4
My guessed rank = 1
My guessed rank = 2
My guessed rank = 6
My guessed rank = 3
My guessed rank = 7
My guessed rank = 0
[2024-02-28 23:44:30,941] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:30,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:30,972] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:30,989] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:31,007] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:31,013] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:31,015] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:31,018] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:31,203] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 6 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-02-28 23:44:31,225] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 6 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-02-28 23:44:31,229] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 6 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-02-28 23:44:31,245] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 6 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-02-28 23:44:31,263] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 6 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-02-28 23:44:31,274] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 6 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-02-28 23:44:31,275] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 6 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-02-28 23:44:31,275] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 6 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
My guessed rank = 4
My guessed rank = 7
My guessed rank = 6
My guessed rank = 1
My guessed rank = 3
My guessed rank = 0
My guessed rank = 2
My guessed rank = 5
[2024-02-28 23:44:36,046] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:36,047] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:36,066] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:36,226] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:36,238] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:36,241] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:36,245] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:36,247] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-02-28 23:44:36,399] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-02-28 23:44:36,399] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-28 23:44:36,399] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-02-28 23:44:36,399] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-28 23:44:36,400] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-02-28 23:44:36,400] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-28 23:44:36,730] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-02-28 23:44:36,731] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-28 23:44:36,738] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-02-28 23:44:36,738] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-28 23:44:36,740] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-02-28 23:44:36,741] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-28 23:44:36,750] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-02-28 23:44:36,750] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-28 23:44:36,754] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-02-28 23:44:36,754] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-28 23:44:36,754] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend ccl
[2024-02-28 23:44:45,048] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 723, num_elems = 68.98B
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   7%|▋         | 1/15 [00:06<01:33,  6.66s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:06<01:33,  6.66s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:06<01:33,  6.66s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:06<01:33,  6.66s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:06<01:33,  6.66s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:06<01:33,  6.66s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:06<01:33,  6.67s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:06<01:33,  6.68s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:13<01:27,  6.71s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:13<01:27,  6.71s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:13<01:27,  6.71s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:13<01:27,  6.72s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:13<01:27,  6.72s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:13<01:27,  6.71s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:13<01:27,  6.72s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:13<01:27,  6.72s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:20<01:20,  6.68s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:20<01:20,  6.68s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:20<01:20,  6.68s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:20<01:20,  6.68s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:20<01:20,  6.68s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:20<01:20,  6.68s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:20<01:20,  6.68s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:20<01:20,  6.68s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:26<01:13,  6.72s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:26<01:13,  6.72s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:26<01:13,  6.72s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:26<01:13,  6.72s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:26<01:13,  6.72s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:26<01:13,  6.72s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:26<01:13,  6.72s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:26<01:13,  6.72s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:33<01:08,  6.80s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:33<01:08,  6.80s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:33<01:08,  6.80s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:33<01:08,  6.80s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:33<01:08,  6.80s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:33<01:08,  6.80s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:33<01:08,  6.80s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:33<01:08,  6.80s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:40<01:01,  6.86s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:40<01:01,  6.86s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:40<01:01,  6.86s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:40<01:01,  6.86s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:40<01:01,  6.86s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:40<01:01,  6.86s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:40<01:01,  6.86s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:40<01:01,  6.86s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:47<00:54,  6.85s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:47<00:54,  6.85s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:47<00:54,  6.85s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:47<00:54,  6.85s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:47<00:54,  6.85s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:47<00:54,  6.85s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:47<00:54,  6.85s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:47<00:54,  6.86s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:54<00:47,  6.76s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:54<00:47,  6.76s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:54<00:47,  6.76s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:54<00:47,  6.76s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:54<00:47,  6.76s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:54<00:47,  6.76s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:54<00:47,  6.76s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:54<00:47,  6.76s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:01<00:40,  6.80s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:01<00:40,  6.80s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:01<00:40,  6.80s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:01<00:40,  6.80s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:01<00:40,  6.80s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:01<00:40,  6.80s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:01<00:40,  6.80s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:01<00:40,  6.80s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:08<00:34,  6.88s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:08<00:34,  6.88s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:08<00:34,  6.88s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:08<00:34,  6.87s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:08<00:34,  6.88s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:08<00:34,  6.88s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:08<00:34,  6.88s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:08<00:34,  6.88s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:15<00:27,  6.94s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:15<00:27,  6.94s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:15<00:27,  6.94s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:15<00:27,  6.94s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:15<00:27,  6.94s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:15<00:27,  6.94s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:15<00:27,  6.94s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:15<00:27,  6.94s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:22<00:20,  6.99s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:22<00:20,  6.99s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:22<00:20,  6.99s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:22<00:20,  6.98s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:22<00:20,  6.99s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:22<00:20,  6.99s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:22<00:20,  6.99s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:22<00:20,  6.99s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:29<00:13,  6.96s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:29<00:13,  6.96s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:29<00:13,  6.96s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:29<00:13,  6.96s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:29<00:13,  6.96s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:29<00:13,  6.96s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:29<00:13,  6.96s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:29<00:13,  6.96s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:35<00:06,  6.87s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:35<00:06,  6.87s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:35<00:06,  6.87s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:35<00:06,  6.87s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:35<00:06,  6.87s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:35<00:06,  6.87s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:35<00:06,  6.87s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:35<00:06,  6.87s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  4.94s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  6.42s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  4.94s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  6.42s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  4.94s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  4.94s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  6.42s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  6.42s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  4.94s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  6.42s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  4.94s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  6.42s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  4.94s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  6.42s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  4.94s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  6.42s/it]
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
prepare all done!
prepare all done!
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2
mll zero 3
mll zero 3
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 8192, padding_idx=0)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): Linear(in_features=8192, out_features=8192, bias=False)
              (k_proj): Linear(in_features=8192, out_features=1024, bias=False)
              (v_proj): Linear(in_features=8192, out_features=1024, bias=False)
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)
              (up_proj): Linear(in_features=8192, out_features=28672, bias=False)
              (down_proj): Linear(in_features=28672, out_features=8192, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=8192, out_features=32000, bias=False)
    )
  )
)
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
prepare all done!
prepare all done!
prepare all done!
prepare all done!
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
prepare all done!
prepare all done!
mll zero 3
mll zero 3
mll zero 3
mll zero 3
mll zero 3
mll zero 3
Parameter Offload: Total persistent parameters: 1318912 in 161 params
  0%|          | 0/800 [00:00<?, ?it/s]/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/800 [00:27<6:07:21, 27.59s/it]  0%|          | 2/800 [00:40<4:13:02, 19.03s/it]                                                 {'loss': 1.2134, 'learning_rate': 0.0004999922894111975, 'epoch': 0.33}
  0%|          | 2/800 [00:40<4:13:02, 19.03s/it]  0%|          | 3/800 [00:53<3:36:37, 16.31s/it]  0%|          | 4/800 [01:06<3:19:02, 15.00s/it]                                                 {'loss': 1.2839, 'learning_rate': 0.0004999691581204152, 'epoch': 0.67}
  0%|          | 4/800 [01:06<3:19:02, 15.00s/it]  1%|          | 5/800 [01:19<3:09:25, 14.30s/it]  1%|          | 6/800 [01:32<3:03:34, 13.87s/it]                                                 {'loss': 1.2602, 'learning_rate': 0.0004999306075545002, 'epoch': 1.0}
  1%|          | 6/800 [01:32<3:03:34, 13.87s/it]  1%|          | 7/800 [01:45<2:59:52, 13.61s/it]  1%|          | 8/800 [01:58<2:57:22, 13.44s/it]                                                 {'loss': 1.2193, 'learning_rate': 0.0004998766400914329, 'epoch': 1.33}
  1%|          | 8/800 [01:58<2:57:22, 13.44s/it]  1%|          | 9/800 [02:11<2:55:35, 13.32s/it]  1%|▏         | 10/800 [02:25<2:54:16, 13.24s/it]                                                  {'loss': 1.1814, 'learning_rate': 0.0004998072590601808, 'epoch': 1.67}
  1%|▏         | 10/800 [02:25<2:54:16, 13.24s/it]  1%|▏         | 11/800 [02:38<2:53:31, 13.20s/it]  2%|▏         | 12/800 [02:51<2:52:46, 13.16s/it]                                                  {'loss': 1.1561, 'learning_rate': 0.0004997224687404926, 'epoch': 2.0}
  2%|▏         | 12/800 [02:51<2:52:46, 13.16s/it]  2%|▏         | 13/800 [03:04<2:52:15, 13.13s/it]  2%|▏         | 14/800 [03:17<2:51:52, 13.12s/it]                                                  {'loss': 1.1476, 'learning_rate': 0.0004996222743626345, 'epoch': 2.33}
  2%|▏         | 14/800 [03:17<2:51:52, 13.12s/it]  2%|▏         | 15/800 [03:30<2:51:33, 13.11s/it]  2%|▏         | 16/800 [03:43<2:51:06, 13.10s/it]                                                  {'loss': 1.1414, 'learning_rate': 0.0004995066821070679, 'epoch': 2.67}
  2%|▏         | 16/800 [03:43<2:51:06, 13.10s/it]  2%|▏         | 17/800 [03:56<2:50:48, 13.09s/it]  2%|▏         | 18/800 [04:09<2:50:34, 13.09s/it]                                                  {'loss': 1.0664, 'learning_rate': 0.0004993756991040675, 'epoch': 3.0}
  2%|▏         | 18/800 [04:09<2:50:34, 13.09s/it]  2%|▏         | 19/800 [04:22<2:50:11, 13.07s/it]  2%|▎         | 20/800 [04:35<2:49:55, 13.07s/it]                                                  {'loss': 1.0721, 'learning_rate': 0.000499229333433282, 'epoch': 3.33}
  2%|▎         | 20/800 [04:35<2:49:55, 13.07s/it]  3%|▎         | 21/800 [04:48<2:49:43, 13.07s/it]  3%|▎         | 22/800 [05:01<2:49:28, 13.07s/it]                                                  {'loss': 0.9956, 'learning_rate': 0.0004990675941232354, 'epoch': 3.67}
  3%|▎         | 22/800 [05:01<2:49:28, 13.07s/it]  3%|▎         | 23/800 [05:15<2:49:16, 13.07s/it]  3%|▎         | 24/800 [05:28<2:48:54, 13.06s/it]                                                  {'loss': 1.0425, 'learning_rate': 0.00049889049115077, 'epoch': 4.0}
  3%|▎         | 24/800 [05:28<2:48:54, 13.06s/it]  3%|▎         | 25/800 [05:41<2:48:40, 13.06s/it]  3%|▎         | 26/800 [05:54<2:48:24, 13.06s/it]                                                  {'loss': 0.9129, 'learning_rate': 0.0004986980354404316, 'epoch': 4.33}
  3%|▎         | 26/800 [05:54<2:48:24, 13.06s/it]  3%|▎         | 27/800 [06:07<2:48:14, 13.06s/it]  4%|▎         | 28/800 [06:20<2:48:10, 13.07s/it]                                                  {'loss': 0.955, 'learning_rate': 0.0004984902388637949, 'epoch': 4.67}
  4%|▎         | 28/800 [06:20<2:48:10, 13.07s/it]  4%|▎         | 29/800 [06:33<2:48:04, 13.08s/it]  4%|▍         | 30/800 [06:46<2:47:52, 13.08s/it]                                                  {'loss': 0.9879, 'learning_rate': 0.0004982671142387316, 'epoch': 5.0}
  4%|▍         | 30/800 [06:46<2:47:52, 13.08s/it]  4%|▍         | 31/800 [06:59<2:47:40, 13.08s/it]  4%|▍         | 32/800 [07:12<2:47:23, 13.08s/it]                                                  {'loss': 0.8952, 'learning_rate': 0.0004980286753286195, 'epoch': 5.33}
  4%|▍         | 32/800 [07:12<2:47:23, 13.08s/it]  4%|▍         | 33/800 [07:25<2:47:09, 13.08s/it]  4%|▍         | 34/800 [07:38<2:46:53, 13.07s/it]                                                  {'loss': 0.8737, 'learning_rate': 0.0004977749368414937, 'epoch': 5.67}
  4%|▍         | 34/800 [07:38<2:46:53, 13.07s/it]  4%|▍         | 35/800 [07:51<2:46:36, 13.07s/it]  4%|▍         | 36/800 [08:04<2:46:20, 13.06s/it]                                                  {'loss': 0.7799, 'learning_rate': 0.0004975059144291394, 'epoch': 6.0}
  4%|▍         | 36/800 [08:04<2:46:20, 13.06s/it]  5%|▍         | 37/800 [08:17<2:46:06, 13.06s/it]  5%|▍         | 38/800 [08:30<2:45:48, 13.06s/it]                                                  {'loss': 0.758, 'learning_rate': 0.0004972216246861262, 'epoch': 6.33}
  5%|▍         | 38/800 [08:31<2:45:48, 13.06s/it]  5%|▍         | 39/800 [08:44<2:45:36, 13.06s/it]  5%|▌         | 40/800 [08:57<2:45:25, 13.06s/it]                                                  {'loss': 0.823, 'learning_rate': 0.0004969220851487844, 'epoch': 6.67}
  5%|▌         | 40/800 [08:57<2:45:25, 13.06s/it]  5%|▌         | 41/800 [09:10<2:45:10, 13.06s/it]  5%|▌         | 42/800 [09:23<2:44:55, 13.05s/it]                                                  {'loss': 0.7485, 'learning_rate': 0.0004966073142941239, 'epoch': 7.0}
  5%|▌         | 42/800 [09:23<2:44:55, 13.05s/it]  5%|▌         | 43/800 [09:36<2:44:44, 13.06s/it]  6%|▌         | 44/800 [09:49<2:44:31, 13.06s/it]                                                  {'loss': 0.6268, 'learning_rate': 0.0004962773315386935, 'epoch': 7.33}
  6%|▌         | 44/800 [09:49<2:44:31, 13.06s/it]  6%|▌         | 45/800 [10:02<2:44:21, 13.06s/it]  6%|▌         | 46/800 [10:15<2:44:04, 13.06s/it]                                                  {'loss': 0.7364, 'learning_rate': 0.000495932157237384, 'epoch': 7.67}
  6%|▌         | 46/800 [10:15<2:44:04, 13.06s/it]  6%|▌         | 47/800 [10:28<2:43:54, 13.06s/it]  6%|▌         | 48/800 [10:41<2:43:31, 13.05s/it]                                                  {'loss': 0.6304, 'learning_rate': 0.0004955718126821722, 'epoch': 8.0}
  6%|▌         | 48/800 [10:41<2:43:31, 13.05s/it]  6%|▌         | 49/800 [10:54<2:43:22, 13.05s/it]  6%|▋         | 50/800 [11:07<2:43:09, 13.05s/it]                                                  {'loss': 0.6831, 'learning_rate': 0.0004951963201008077, 'epoch': 8.33}
  6%|▋         | 50/800 [11:07<2:43:09, 13.05s/it]  6%|▋         | 51/800 [11:20<2:43:04, 13.06s/it]  6%|▋         | 52/800 [11:33<2:42:52, 13.07s/it]                                                  {'loss': 0.5251, 'learning_rate': 0.0004948057026554415, 'epoch': 8.67}
  6%|▋         | 52/800 [11:33<2:42:52, 13.07s/it]  7%|▋         | 53/800 [11:46<2:42:38, 13.06s/it]  7%|▋         | 54/800 [11:59<2:42:24, 13.06s/it]                                                  {'loss': 0.5093, 'learning_rate': 0.0004943999844411977, 'epoch': 9.0}
  7%|▋         | 54/800 [11:59<2:42:24, 13.06s/it]  7%|▋         | 55/800 [12:13<2:42:15, 13.07s/it]  7%|▋         | 56/800 [12:26<2:42:07, 13.07s/it]                                                  {'loss': 0.4471, 'learning_rate': 0.0004939791904846869, 'epoch': 9.33}
  7%|▋         | 56/800 [12:26<2:42:07, 13.07s/it]  7%|▋         | 57/800 [12:39<2:41:56, 13.08s/it]  7%|▋         | 58/800 [12:52<2:41:42, 13.08s/it]                                                  {'loss': 0.4488, 'learning_rate': 0.0004935433467424624, 'epoch': 9.67}
  7%|▋         | 58/800 [12:52<2:41:42, 13.08s/it]  7%|▋         | 59/800 [13:05<2:41:33, 13.08s/it]  8%|▊         | 60/800 [13:18<2:41:21, 13.08s/it]                                                  {'loss': 0.4635, 'learning_rate': 0.0004930924800994192, 'epoch': 10.0}
  8%|▊         | 60/800 [13:18<2:41:21, 13.08s/it]  8%|▊         | 61/800 [13:31<2:41:07, 13.08s/it]  8%|▊         | 62/800 [13:44<2:40:48, 13.07s/it]                                                  {'loss': 0.439, 'learning_rate': 0.0004926266183671356, 'epoch': 10.33}
  8%|▊         | 62/800 [13:44<2:40:48, 13.07s/it]  8%|▊         | 63/800 [13:57<2:40:39, 13.08s/it]  8%|▊         | 64/800 [14:10<2:40:25, 13.08s/it]                                                  {'loss': 0.3691, 'learning_rate': 0.0004921457902821578, 'epoch': 10.67}
  8%|▊         | 64/800 [14:10<2:40:25, 13.08s/it]  8%|▊         | 65/800 [14:23<2:40:12, 13.08s/it]  8%|▊         | 66/800 [14:36<2:39:55, 13.07s/it]                                                  {'loss': 0.2459, 'learning_rate': 0.0004916500255042268, 'epoch': 11.0}
  8%|▊         | 66/800 [14:36<2:39:55, 13.07s/it]  8%|▊         | 67/800 [14:49<2:39:45, 13.08s/it]  8%|▊         | 68/800 [15:03<2:39:33, 13.08s/it]                                                  {'loss': 0.2333, 'learning_rate': 0.0004911393546144495, 'epoch': 11.33}
  8%|▊         | 68/800 [15:03<2:39:33, 13.08s/it]  9%|▊         | 69/800 [15:16<2:39:17, 13.07s/it]  9%|▉         | 70/800 [15:29<2:39:00, 13.07s/it]                                                  {'loss': 0.2589, 'learning_rate': 0.0004906138091134118, 'epoch': 11.67}
  9%|▉         | 70/800 [15:29<2:39:00, 13.07s/it]  9%|▉         | 71/800 [15:42<2:38:56, 13.08s/it]  9%|▉         | 72/800 [15:55<2:38:40, 13.08s/it]                                                  {'loss': 0.2517, 'learning_rate': 0.0004900734214192358, 'epoch': 12.0}
  9%|▉         | 72/800 [15:55<2:38:40, 13.08s/it]  9%|▉         | 73/800 [16:08<2:38:21, 13.07s/it]  9%|▉         | 74/800 [16:21<2:37:58, 13.06s/it]                                                  {'loss': 0.1997, 'learning_rate': 0.0004895182248655798, 'epoch': 12.33}
  9%|▉         | 74/800 [16:21<2:37:58, 13.06s/it]  9%|▉         | 75/800 [16:34<2:37:48, 13.06s/it] 10%|▉         | 76/800 [16:47<2:37:32, 13.06s/it]                                                  {'loss': 0.1605, 'learning_rate': 0.0004889482536995825, 'epoch': 12.67}
 10%|▉         | 76/800 [16:47<2:37:32, 13.06s/it] 10%|▉         | 77/800 [17:00<2:37:23, 13.06s/it] 10%|▉         | 78/800 [17:13<2:37:08, 13.06s/it]                                                  {'loss': 0.1699, 'learning_rate': 0.0004883635430797502, 'epoch': 13.0}
 10%|▉         | 78/800 [17:13<2:37:08, 13.06s/it] 10%|▉         | 79/800 [17:26<2:37:04, 13.07s/it] 10%|█         | 80/800 [17:39<2:36:49, 13.07s/it]                                                  {'loss': 0.1348, 'learning_rate': 0.0004877641290737884, 'epoch': 13.33}
 10%|█         | 80/800 [17:39<2:36:49, 13.07s/it] 10%|█         | 81/800 [17:52<2:36:35, 13.07s/it] 10%|█         | 82/800 [18:05<2:36:21, 13.07s/it]                                                  {'loss': 0.1381, 'learning_rate': 0.0004871500486563761, 'epoch': 13.67}
 10%|█         | 82/800 [18:05<2:36:21, 13.07s/it] 10%|█         | 83/800 [18:19<2:36:13, 13.07s/it] 10%|█         | 84/800 [18:32<2:36:02, 13.08s/it]                                                  {'loss': 0.0978, 'learning_rate': 0.00048652133970688633, 'epoch': 14.0}
 10%|█         | 84/800 [18:32<2:36:02, 13.08s/it] 11%|█         | 85/800 [18:45<2:35:44, 13.07s/it] 11%|█         | 86/800 [18:58<2:35:29, 13.07s/it]                                                  {'loss': 0.1034, 'learning_rate': 0.0004858780410070484, 'epoch': 14.33}
 11%|█         | 86/800 [18:58<2:35:29, 13.07s/it] 11%|█         | 87/800 [19:11<2:35:18, 13.07s/it] 11%|█         | 88/800 [19:24<2:35:05, 13.07s/it]                                                  {'loss': 0.0826, 'learning_rate': 0.0004852201922385564, 'epoch': 14.67}
 11%|█         | 88/800 [19:24<2:35:05, 13.07s/it] 11%|█         | 89/800 [19:37<2:34:50, 13.07s/it] 11%|█▏        | 90/800 [19:50<2:34:40, 13.07s/it]                                                  {'loss': 0.0869, 'learning_rate': 0.0004845478339806211, 'epoch': 15.0}
 11%|█▏        | 90/800 [19:50<2:34:40, 13.07s/it] 11%|█▏        | 91/800 [20:03<2:34:26, 13.07s/it] 12%|█▏        | 92/800 [20:16<2:34:15, 13.07s/it]                                                  {'loss': 0.0595, 'learning_rate': 0.00048386100770746686, 'epoch': 15.33}
 12%|█▏        | 92/800 [20:16<2:34:15, 13.07s/it] 12%|█▏        | 93/800 [20:29<2:34:01, 13.07s/it] 12%|█▏        | 94/800 [20:42<2:33:50, 13.07s/it]                                                  {'loss': 0.0559, 'learning_rate': 0.0004831597557857735, 'epoch': 15.67}
 12%|█▏        | 94/800 [20:42<2:33:50, 13.07s/it] 12%|█▏        | 95/800 [20:55<2:33:40, 13.08s/it] 12%|█▏        | 96/800 [21:08<2:33:24, 13.08s/it]                                                  {'loss': 0.0551, 'learning_rate': 0.00048244412147206283, 'epoch': 16.0}
 12%|█▏        | 96/800 [21:09<2:33:24, 13.08s/it] 12%|█▏        | 97/800 [21:22<2:33:17, 13.08s/it] 12%|█▏        | 98/800 [21:35<2:33:06, 13.09s/it]                                                  {'loss': 0.0305, 'learning_rate': 0.0004817141489100302, 'epoch': 16.33}
 12%|█▏        | 98/800 [21:35<2:33:06, 13.09s/it] 12%|█▏        | 99/800 [21:48<2:32:50, 13.08s/it] 12%|█▎        | 100/800 [22:01<2:32:35, 13.08s/it]                                                   {'loss': 0.0459, 'learning_rate': 0.0004809698831278217, 'epoch': 16.67}
 12%|█▎        | 100/800 [22:01<2:32:35, 13.08s/it] 13%|█▎        | 101/800 [22:14<2:32:21, 13.08s/it] 13%|█▎        | 102/800 [22:27<2:32:03, 13.07s/it]                                                   {'loss': 0.0381, 'learning_rate': 0.0004802113700352566, 'epoch': 17.0}
 13%|█▎        | 102/800 [22:27<2:32:03, 13.07s/it] 13%|█▎        | 103/800 [22:40<2:31:55, 13.08s/it] 13%|█▎        | 104/800 [22:53<2:31:46, 13.08s/it]                                                   {'loss': 0.029, 'learning_rate': 0.00047943865642099525, 'epoch': 17.33}
 13%|█▎        | 104/800 [22:53<2:31:46, 13.08s/it] 13%|█▎        | 105/800 [23:06<2:31:28, 13.08s/it] 13%|█▎        | 106/800 [23:19<2:31:10, 13.07s/it]                                                   {'loss': 0.0261, 'learning_rate': 0.0004786517899496534, 'epoch': 17.67}
 13%|█▎        | 106/800 [23:19<2:31:10, 13.07s/it] 13%|█▎        | 107/800 [23:32<2:31:04, 13.08s/it] 14%|█▎        | 108/800 [23:45<2:30:46, 13.07s/it]                                                   {'loss': 0.032, 'learning_rate': 0.0004778508191588613, 'epoch': 18.0}
 14%|█▎        | 108/800 [23:45<2:30:46, 13.07s/it] 14%|█▎        | 109/800 [23:59<2:30:42, 13.09s/it] 14%|█▍        | 110/800 [24:12<2:30:23, 13.08s/it]                                                   {'loss': 0.0167, 'learning_rate': 0.00047703579345627036, 'epoch': 18.33}
 14%|█▍        | 110/800 [24:12<2:30:23, 13.08s/it] 14%|█▍        | 111/800 [24:25<2:30:10, 13.08s/it] 14%|█▍        | 112/800 [24:38<2:30:01, 13.08s/it]                                                   {'loss': 0.0212, 'learning_rate': 0.0004762067631165049, 'epoch': 18.67}
 14%|█▍        | 112/800 [24:38<2:30:01, 13.08s/it] 14%|█▍        | 113/800 [24:51<2:29:45, 13.08s/it] 14%|█▍        | 114/800 [25:04<2:29:26, 13.07s/it]                                                   {'loss': 0.0218, 'learning_rate': 0.00047536377927806143, 'epoch': 19.0}
 14%|█▍        | 114/800 [25:04<2:29:26, 13.07s/it] 14%|█▍        | 115/800 [25:17<2:29:14, 13.07s/it] 14%|█▍        | 116/800 [25:30<2:28:59, 13.07s/it]                                                   {'loss': 0.013, 'learning_rate': 0.0004745068939401539, 'epoch': 19.33}
 14%|█▍        | 116/800 [25:30<2:28:59, 13.07s/it] 15%|█▍        | 117/800 [25:43<2:28:45, 13.07s/it] 15%|█▍        | 118/800 [25:56<2:28:29, 13.06s/it]                                                   {'loss': 0.015, 'learning_rate': 0.00047363615995950624, 'epoch': 19.67}
 15%|█▍        | 118/800 [25:56<2:28:29, 13.06s/it] 15%|█▍        | 119/800 [26:09<2:28:17, 13.07s/it] 15%|█▌        | 120/800 [26:22<2:28:05, 13.07s/it]                                                   {'loss': 0.02, 'learning_rate': 0.00047275163104709196, 'epoch': 20.0}
 15%|█▌        | 120/800 [26:22<2:28:05, 13.07s/it] 15%|█▌        | 121/800 [26:35<2:27:56, 13.07s/it] 15%|█▌        | 122/800 [26:48<2:27:45, 13.08s/it]                                                   {'loss': 0.0116, 'learning_rate': 0.00047185336176482084, 'epoch': 20.33}
 15%|█▌        | 122/800 [26:48<2:27:45, 13.08s/it] 15%|█▌        | 123/800 [27:02<2:27:27, 13.07s/it] 16%|█▌        | 124/800 [27:15<2:27:16, 13.07s/it]                                                   {'loss': 0.0103, 'learning_rate': 0.0004709414075221734, 'epoch': 20.67}
 16%|█▌        | 124/800 [27:15<2:27:16, 13.07s/it] 16%|█▌        | 125/800 [27:28<2:26:58, 13.06s/it] 16%|█▌        | 126/800 [27:41<2:26:43, 13.06s/it]                                                   {'loss': 0.0136, 'learning_rate': 0.000470015824572783, 'epoch': 21.0}
 16%|█▌        | 126/800 [27:41<2:26:43, 13.06s/it] 16%|█▌        | 127/800 [27:54<2:26:38, 13.07s/it] 16%|█▌        | 128/800 [28:07<2:26:26, 13.07s/it]                                                   {'loss': 0.0111, 'learning_rate': 0.0004690766700109659, 'epoch': 21.33}
 16%|█▌        | 128/800 [28:07<2:26:26, 13.07s/it] 16%|█▌        | 129/800 [28:20<2:26:15, 13.08s/it] 16%|█▋        | 130/800 [28:33<2:26:03, 13.08s/it]                                                   {'loss': 0.0081, 'learning_rate': 0.0004681240017681993, 'epoch': 21.67}
 16%|█▋        | 130/800 [28:33<2:26:03, 13.08s/it] 16%|█▋        | 131/800 [28:46<2:25:48, 13.08s/it] 16%|█▋        | 132/800 [28:59<2:25:38, 13.08s/it]                                                   {'loss': 0.0093, 'learning_rate': 0.00046715787860954785, 'epoch': 22.0}
 16%|█▋        | 132/800 [28:59<2:25:38, 13.08s/it] 17%|█▋        | 133/800 [29:12<2:25:29, 13.09s/it] 17%|█▋        | 134/800 [29:25<2:25:11, 13.08s/it]                                                   {'loss': 0.0086, 'learning_rate': 0.0004661783601300388, 'epoch': 22.33}
 17%|█▋        | 134/800 [29:25<2:25:11, 13.08s/it] 17%|█▋        | 135/800 [29:38<2:25:01, 13.08s/it] 17%|█▋        | 136/800 [29:52<2:24:46, 13.08s/it]                                                   {'loss': 0.0061, 'learning_rate': 0.0004651855067509859, 'epoch': 22.67}
 17%|█▋        | 136/800 [29:52<2:24:46, 13.08s/it] 17%|█▋        | 137/800 [30:05<2:24:33, 13.08s/it] 17%|█▋        | 138/800 [30:18<2:24:13, 13.07s/it]                                                   {'loss': 0.006, 'learning_rate': 0.00046417937971626245, 'epoch': 23.0}
 17%|█▋        | 138/800 [30:18<2:24:13, 13.07s/it] 17%|█▋        | 139/800 [30:31<2:24:03, 13.08s/it] 18%|█▊        | 140/800 [30:44<2:23:53, 13.08s/it]                                                   {'loss': 0.0053, 'learning_rate': 0.00046316004108852305, 'epoch': 23.33}
 18%|█▊        | 140/800 [30:44<2:23:53, 13.08s/it] 18%|█▊        | 141/800 [30:57<2:23:45, 13.09s/it] 18%|█▊        | 142/800 [31:10<2:23:27, 13.08s/it]                                                   {'loss': 0.0051, 'learning_rate': 0.00046212755374537594, 'epoch': 23.67}
 18%|█▊        | 142/800 [31:10<2:23:27, 13.08s/it] 18%|█▊        | 143/800 [31:23<2:23:14, 13.08s/it] 18%|█▊        | 144/800 [31:36<2:23:01, 13.08s/it]                                                   {'loss': 0.0058, 'learning_rate': 0.00046108198137550377, 'epoch': 24.0}
 18%|█▊        | 144/800 [31:36<2:23:01, 13.08s/it] 18%|█▊        | 145/800 [31:49<2:22:46, 13.08s/it] 18%|█▊        | 146/800 [32:02<2:22:58, 13.12s/it]                                                   {'loss': 0.0056, 'learning_rate': 0.00046002338847473545, 'epoch': 24.33}
 18%|█▊        | 146/800 [32:02<2:22:58, 13.12s/it] 18%|█▊        | 147/800 [32:16<2:22:42, 13.11s/it] 18%|█▊        | 148/800 [32:29<2:22:24, 13.10s/it]                                                   {'loss': 0.0045, 'learning_rate': 0.0004589518403420676, 'epoch': 24.67}
 18%|█▊        | 148/800 [32:29<2:22:24, 13.10s/it] 19%|█▊        | 149/800 [32:42<2:22:08, 13.10s/it] 19%|█▉        | 150/800 [32:55<2:21:44, 13.08s/it]                                                   {'loss': 0.0032, 'learning_rate': 0.00045786740307563633, 'epoch': 25.0}
 19%|█▉        | 150/800 [32:55<2:21:44, 13.08s/it] 19%|█▉        | 151/800 [33:08<2:21:35, 13.09s/it] 19%|█▉        | 152/800 [33:21<2:21:23, 13.09s/it]                                                   {'loss': 0.0034, 'learning_rate': 0.00045677014356864043, 'epoch': 25.33}
 19%|█▉        | 152/800 [33:21<2:21:23, 13.09s/it] 19%|█▉        | 153/800 [33:34<2:21:07, 13.09s/it] 19%|█▉        | 154/800 [33:47<2:20:52, 13.08s/it]                                                   {'loss': 0.0046, 'learning_rate': 0.00045566012950521497, 'epoch': 25.67}
 19%|█▉        | 154/800 [33:47<2:20:52, 13.08s/it] 19%|█▉        | 155/800 [34:00<2:20:42, 13.09s/it] 20%|█▉        | 156/800 [34:13<2:20:29, 13.09s/it]                                                   {'loss': 0.003, 'learning_rate': 0.0004545374293562559, 'epoch': 26.0}
 20%|█▉        | 156/800 [34:13<2:20:29, 13.09s/it] 20%|█▉        | 157/800 [34:26<2:20:20, 13.10s/it] 20%|█▉        | 158/800 [34:40<2:20:05, 13.09s/it]                                                   {'loss': 0.0023, 'learning_rate': 0.0004534021123751968, 'epoch': 26.33}
 20%|█▉        | 158/800 [34:40<2:20:05, 13.09s/it] 20%|█▉        | 159/800 [34:53<2:19:51, 13.09s/it] 20%|██        | 160/800 [35:06<2:19:39, 13.09s/it]                                                   {'loss': 0.0026, 'learning_rate': 0.0004522542485937369, 'epoch': 26.67}
 20%|██        | 160/800 [35:06<2:19:39, 13.09s/it] 20%|██        | 161/800 [35:19<2:19:30, 13.10s/it] 20%|██        | 162/800 [35:32<2:19:15, 13.10s/it]                                                   {'loss': 0.0047, 'learning_rate': 0.0004510939088175211, 'epoch': 27.0}
 20%|██        | 162/800 [35:32<2:19:15, 13.10s/it] 20%|██        | 163/800 [35:45<2:19:06, 13.10s/it] 20%|██        | 164/800 [35:58<2:18:53, 13.10s/it]                                                   {'loss': 0.0032, 'learning_rate': 0.0004499211646217727, 'epoch': 27.33}
 20%|██        | 164/800 [35:58<2:18:53, 13.10s/it] 21%|██        | 165/800 [36:11<2:18:35, 13.10s/it] 21%|██        | 166/800 [36:24<2:18:24, 13.10s/it]                                                   {'loss': 0.0024, 'learning_rate': 0.00044873608834687754, 'epoch': 27.67}
 21%|██        | 166/800 [36:24<2:18:24, 13.10s/it] 21%|██        | 167/800 [36:37<2:18:05, 13.09s/it] 21%|██        | 168/800 [36:50<2:17:54, 13.09s/it]                                                   {'loss': 0.0024, 'learning_rate': 0.0004475387530939226, 'epoch': 28.0}
 21%|██        | 168/800 [36:51<2:17:54, 13.09s/it] 21%|██        | 169/800 [37:04<2:17:44, 13.10s/it] 21%|██▏       | 170/800 [37:17<2:17:31, 13.10s/it]                                                   {'loss': 0.0025, 'learning_rate': 0.0004463292327201862, 'epoch': 28.33}
 21%|██▏       | 170/800 [37:17<2:17:31, 13.10s/it] 21%|██▏       | 171/800 [37:30<2:17:10, 13.09s/it]slurmstepd: error: *** STEP 190353.2 ON compute23 CANCELLED AT 2024-02-29T00:24:00 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 0 PID 136971 RUNNING AT compute23
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 1 PID 136972 RUNNING AT compute23
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 2 PID 136973 RUNNING AT compute23
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 3 PID 136974 RUNNING AT compute23
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 4 PID 136975 RUNNING AT compute23
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 5 PID 136976 RUNNING AT compute23
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 6 PID 136977 RUNNING AT compute23
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 7 PID 136978 RUNNING AT compute23
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================
