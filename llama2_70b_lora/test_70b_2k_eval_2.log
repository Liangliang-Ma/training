[2024-03-04 00:31:24,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
could not open any host key
ssh_keysign: no reply
sign using hostkey ssh-ed25519 SHA256:9AX4dFSSpcAMAJhyYK5BBAhA6FO0voQ4qjlON/fIicA failed
[2024-03-04 00:31:26,014] [INFO] [runner.py:463:main] Using IP address of 172.16.1.34 for node compute34
[2024-03-04 00:31:26,019] [INFO] [runner.py:568:main] cmd = mpirun -n 8 -ppn 8 -hostfile hostfile_mpich -genv PYTHONSTARTUP=/etc/pythonstart -genv PYTHONPATH=/home/maliangl/ft/reference/llama2_70b_lora -genv MASTER_ADDR 172.16.1.34 -genv MASTER_PORT 29500 -genv WORLD_SIZE 8 -genv LOCAL_SIZE 8 -hosts compute34 /home/maliangl/miniconda3/envs/ft/bin/python -u /home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/deepspeed/launcher/launcher_helper.py --launcher mpich scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 10 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
My guessed rank = 7
My guessed rank = 5
My guessed rank = 1
My guessed rank = 0
My guessed rank = 3
My guessed rank = 2
My guessed rank = 4
My guessed rank = 6
[2024-03-04 00:31:31,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:31,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:31,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:31,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:31,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:31,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:31,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:31,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:31,823] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 10 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-03-04 00:31:31,823] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 10 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-03-04 00:31:31,823] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 10 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-03-04 00:31:31,823] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 10 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-03-04 00:31:31,823] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 10 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-03-04 00:31:31,823] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 10 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-03-04 00:31:31,823] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 10 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
[2024-03-04 00:31:31,823] [INFO] [launcher_helper.py:101:main] launcher_helper cmd = /home/maliangl/miniconda3/envs/ft/bin/python -u scripts/train.py --model_path /scratch/users/maliangl/Llama-2-70b-hf --dataset_name tau/scrolls --dataset_config_name gov_report --max_seq_len 2048 --bf16 True --logging_steps 2 --eval_steps 10 --save_steps 999 --output_dir ./results/llama-70b_scrolls_gov_report_r16_666 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --learning_rate 5e-4 --warmup_ratio 0 --use_gradient_checkpointing True --target_eval_loss 0.925 --use_peft_lora True --lora_r 16 --lora_alpha 16 --lora_dropout 0.1 --max_steps 800 --seed 666 --lora_target_modules qkv_proj,o_proj
My guessed rank = 4
My guessed rank = 5
My guessed rank = 0
My guessed rank = 1
My guessed rank = 2
My guessed rank = 7
My guessed rank = 6
My guessed rank = 3
[2024-03-04 00:31:37,746] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:37,746] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:37,746] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:37,746] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:37,746] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:37,746] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:37,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:37,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to xpu (auto detect)
[2024-03-04 00:31:38,616] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-03-04 00:31:38,617] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-04 00:31:38,617] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-03-04 00:31:38,617] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend ccl
[2024-03-04 00:31:38,617] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-04 00:31:38,619] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-03-04 00:31:38,619] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-04 00:31:38,630] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-03-04 00:31:38,631] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-04 00:31:38,631] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-03-04 00:31:38,631] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-04 00:31:38,633] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-03-04 00:31:38,633] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-04 00:31:38,648] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-03-04 00:31:38,649] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-04 00:31:38,649] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-03-04 00:31:38,649] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-04 00:31:47,897] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 723, num_elems = 68.98B
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   7%|▋         | 1/15 [00:11<02:39, 11.39s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:11<02:39, 11.39s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:11<02:39, 11.39s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:11<02:39, 11.37s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:11<02:39, 11.39s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:11<02:39, 11.39s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:11<02:39, 11.39s/it]Loading checkpoint shards:   7%|▋         | 1/15 [00:11<02:39, 11.40s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:22<02:26, 11.23s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:22<02:26, 11.23s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:22<02:25, 11.23s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:22<02:26, 11.23s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:22<02:26, 11.23s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:22<02:26, 11.23s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:22<02:26, 11.23s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:22<02:26, 11.24s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:34<02:17, 11.43s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:34<02:17, 11.43s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:34<02:17, 11.43s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:34<02:17, 11.43s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:34<02:17, 11.43s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:34<02:17, 11.43s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:34<02:17, 11.43s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:34<02:17, 11.44s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:45<02:03, 11.24s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:45<02:03, 11.25s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:45<02:03, 11.24s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:45<02:03, 11.24s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:45<02:03, 11.25s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:45<02:03, 11.25s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:45<02:03, 11.25s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:45<02:03, 11.25s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:56<01:51, 11.13s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:56<01:51, 11.14s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:56<01:51, 11.14s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:56<01:51, 11.14s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:56<01:51, 11.14s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:56<01:51, 11.14s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:56<01:51, 11.14s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:56<01:51, 11.14s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:06<01:38, 10.97s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:06<01:38, 10.97s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:06<01:38, 10.97s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:06<01:38, 10.97s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:06<01:38, 10.97s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:06<01:38, 10.97s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:06<01:38, 10.97s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:06<01:38, 10.97s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:18<01:30, 11.32s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:18<01:30, 11.32s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:18<01:30, 11.32s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:18<01:30, 11.32s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:18<01:30, 11.32s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:18<01:30, 11.33s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:18<01:30, 11.33s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:18<01:30, 11.33s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:29<01:18, 11.26s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:29<01:18, 11.26s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:29<01:18, 11.26s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:29<01:18, 11.26s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:29<01:18, 11.26s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:29<01:18, 11.26s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:29<01:18, 11.26s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:29<01:18, 11.26s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:41<01:07, 11.30s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:41<01:07, 11.30s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:41<01:07, 11.30s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:41<01:07, 11.30s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:41<01:07, 11.30s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:41<01:07, 11.31s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:41<01:07, 11.31s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:41<01:07, 11.30s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:52<00:56, 11.26s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:52<00:56, 11.26s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:52<00:56, 11.26s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:52<00:56, 11.26s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:52<00:56, 11.26s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:52<00:56, 11.26s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:52<00:56, 11.27s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:52<00:56, 11.26s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:03<00:45, 11.27s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:03<00:45, 11.27s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:03<00:45, 11.27s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:03<00:45, 11.27s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:03<00:45, 11.27s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:03<00:45, 11.27s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:03<00:45, 11.27s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:03<00:45, 11.28s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:15<00:34, 11.36s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:15<00:34, 11.36s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:15<00:34, 11.36s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:15<00:34, 11.36s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:15<00:34, 11.36s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:15<00:34, 11.36s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:15<00:34, 11.36s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:15<00:34, 11.36s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [02:26<00:22, 11.44s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [02:26<00:22, 11.44s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [02:26<00:22, 11.44s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [02:26<00:22, 11.44s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [02:26<00:22, 11.44s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [02:26<00:22, 11.44s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [02:26<00:22, 11.44s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [02:26<00:22, 11.44s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [02:38<00:11, 11.39s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [02:38<00:11, 11.39s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [02:38<00:11, 11.39s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [02:38<00:11, 11.39s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [02:38<00:11, 11.39s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [02:38<00:11, 11.39s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [02:38<00:11, 11.39s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [02:38<00:11, 11.39s/it]Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00, 10.59s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00, 10.59s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00, 10.59s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00, 10.59s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00, 10.59s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00, 10.59s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00, 10.59s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 15/15 [02:38<00:00, 10.59s/it]
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Before packing, Size of the train set: 68. Size of the validation set: 4
Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2Size of the train set: 46. Size of the validation set: 2

Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2
Size of the train set: 46. Size of the validation set: 2
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 8192, padding_idx=0)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): Linear(in_features=8192, out_features=8192, bias=False)
              (k_proj): Linear(in_features=8192, out_features=1024, bias=False)
              (v_proj): Linear(in_features=8192, out_features=1024, bias=False)
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)
              (up_proj): Linear(in_features=8192, out_features=28672, bias=False)
              (down_proj): Linear(in_features=28672, out_features=8192, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=8192, out_features=32000, bias=False)
    )
  )
)
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
prepare all done!
prepare all done!
prepare all done!
prepare all done!
prepare all done!
prepare all done!
prepare all done!
mll zero 3
mll zero 3
mll zero 3
mll zero 3
mll zero 3
mll zero 3
mll zero 3
Before packing, Size of the train set: 68. Size of the validation set: 4
Size of the train set: 46. Size of the validation set: 2
trainable params: 20,971,520 || all params: 68,997,619,712 || trainable%: 0.03039455576516454
prepare all done!
mll zero 3
Parameter Offload: Total persistent parameters: 1318912 in 161 params
  0%|          | 0/800 [00:00<?, ?it/s]/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/maliangl/miniconda3/envs/ft/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/800 [00:27<6:08:58, 27.71s/it]  0%|          | 2/800 [00:40<4:12:46, 19.01s/it]                                                 {'loss': 1.2134, 'learning_rate': 0.0004999922894111975, 'epoch': 0.33}
  0%|          | 2/800 [00:40<4:12:46, 19.01s/it]  0%|          | 3/800 [00:53<3:35:19, 16.21s/it]  0%|          | 4/800 [01:06<3:17:29, 14.89s/it]                                                 {'loss': 1.2842, 'learning_rate': 0.0004999691581204152, 'epoch': 0.67}
  0%|          | 4/800 [01:06<3:17:29, 14.89s/it]  1%|          | 5/800 [01:19<3:07:33, 14.16s/it]  1%|          | 6/800 [01:32<3:01:26, 13.71s/it]                                                 {'loss': 1.2605, 'learning_rate': 0.0004999306075545002, 'epoch': 1.0}
  1%|          | 6/800 [01:32<3:01:26, 13.71s/it]  1%|          | 7/800 [01:44<2:57:35, 13.44s/it]  1%|          | 8/800 [01:57<2:54:55, 13.25s/it]                                                 {'loss': 1.2195, 'learning_rate': 0.0004998766400914329, 'epoch': 1.33}
  1%|          | 8/800 [01:57<2:54:55, 13.25s/it]  1%|          | 9/800 [02:10<2:53:05, 13.13s/it]  1%|▏         | 10/800 [02:23<2:51:43, 13.04s/it]                                                  {'loss': 1.1818, 'learning_rate': 0.0004998072590601808, 'epoch': 1.67}
  1%|▏         | 10/800 [02:23<2:51:43, 13.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                  
                                     [A{'eval_loss': 0.43290942907333374, 'eval_runtime': 5.0393, 'eval_samples_per_second': 0.397, 'eval_steps_per_second': 0.198, 'epoch': 1.67}
  1%|▏         | 10/800 [02:28<2:51:43, 13.04s/it]
100%|██████████| 1/1 [00:00<00:00, 44.50it/s][A
                                             [A  1%|▏         | 11/800 [02:41<3:11:24, 14.56s/it]  2%|▏         | 12/800 [02:54<3:04:22, 14.04s/it]                                                  {'loss': 1.1563, 'learning_rate': 0.0004997224687404926, 'epoch': 2.0}
  2%|▏         | 12/800 [02:54<3:04:22, 14.04s/it]  2%|▏         | 13/800 [03:07<2:59:30, 13.69s/it]  2%|▏         | 14/800 [03:20<2:56:03, 13.44s/it]                                                  {'loss': 1.1207, 'learning_rate': 0.0004996222743626345, 'epoch': 2.33}
  2%|▏         | 14/800 [03:20<2:56:03, 13.44s/it]  2%|▏         | 15/800 [03:32<2:53:34, 13.27s/it]  2%|▏         | 16/800 [03:45<2:51:44, 13.14s/it]                                                  {'loss': 1.1327, 'learning_rate': 0.0004995066821070679, 'epoch': 2.67}
  2%|▏         | 16/800 [03:45<2:51:44, 13.14s/it]  2%|▏         | 17/800 [03:58<2:50:26, 13.06s/it]  2%|▏         | 18/800 [04:11<2:49:25, 13.00s/it]                                                  {'loss': 1.0773, 'learning_rate': 0.0004993756991040675, 'epoch': 3.0}
  2%|▏         | 18/800 [04:11<2:49:25, 13.00s/it]  2%|▏         | 19/800 [04:24<2:48:41, 12.96s/it]  2%|▎         | 20/800 [04:37<2:48:04, 12.93s/it]                                                  {'loss': 1.0065, 'learning_rate': 0.000499229333433282, 'epoch': 3.33}
  2%|▎         | 20/800 [04:37<2:48:04, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                  
                                     [A{'eval_loss': 0.41933414340019226, 'eval_runtime': 4.9808, 'eval_samples_per_second': 0.402, 'eval_steps_per_second': 0.201, 'epoch': 3.33}
  2%|▎         | 20/800 [04:42<2:48:04, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 60.24it/s][A
                                             [A  3%|▎         | 21/800 [04:55<3:07:03, 14.41s/it]  3%|▎         | 22/800 [05:07<3:00:46, 13.94s/it]                                                  {'loss': 1.0751, 'learning_rate': 0.0004990675941232354, 'epoch': 3.67}
  3%|▎         | 22/800 [05:07<3:00:46, 13.94s/it]  3%|▎         | 23/800 [05:20<2:56:26, 13.62s/it]  3%|▎         | 24/800 [05:33<2:53:14, 13.40s/it]                                                  {'loss': 1.0691, 'learning_rate': 0.00049889049115077, 'epoch': 4.0}
  3%|▎         | 24/800 [05:33<2:53:14, 13.40s/it]  3%|▎         | 25/800 [05:46<2:51:00, 13.24s/it]  3%|▎         | 26/800 [05:59<2:49:19, 13.13s/it]                                                  {'loss': 1.0007, 'learning_rate': 0.0004986980354404316, 'epoch': 4.33}
  3%|▎         | 26/800 [05:59<2:49:19, 13.13s/it]  3%|▎         | 27/800 [06:12<2:48:06, 13.05s/it]  4%|▎         | 28/800 [06:25<2:47:11, 12.99s/it]                                                  {'loss': 0.9817, 'learning_rate': 0.0004984902388637949, 'epoch': 4.67}
  4%|▎         | 28/800 [06:25<2:47:11, 12.99s/it]  4%|▎         | 29/800 [06:38<2:46:28, 12.96s/it]  4%|▍         | 30/800 [06:50<2:45:53, 12.93s/it]                                                  {'loss': 0.8838, 'learning_rate': 0.0004982671142387316, 'epoch': 5.0}
  4%|▍         | 30/800 [06:50<2:45:53, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                  
                                     [A{'eval_loss': 0.4323253333568573, 'eval_runtime': 4.9854, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 5.0}
  4%|▍         | 30/800 [06:55<2:45:53, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 45.49it/s][A
                                             [A  4%|▍         | 31/800 [07:08<3:04:43, 14.41s/it]  4%|▍         | 32/800 [07:21<2:58:33, 13.95s/it]                                                  {'loss': 0.8297, 'learning_rate': 0.0004980286753286195, 'epoch': 5.33}
  4%|▍         | 32/800 [07:21<2:58:33, 13.95s/it]  4%|▍         | 33/800 [07:34<2:54:08, 13.62s/it]  4%|▍         | 34/800 [07:47<2:50:57, 13.39s/it]                                                  {'loss': 0.9543, 'learning_rate': 0.0004977749368414937, 'epoch': 5.67}
  4%|▍         | 34/800 [07:47<2:50:57, 13.39s/it]  4%|▍         | 35/800 [08:00<2:48:44, 13.23s/it]  4%|▍         | 36/800 [08:13<2:47:06, 13.12s/it]                                                  {'loss': 0.8301, 'learning_rate': 0.0004975059144291394, 'epoch': 6.0}
  4%|▍         | 36/800 [08:13<2:47:06, 13.12s/it]  5%|▍         | 37/800 [08:25<2:45:52, 13.04s/it]  5%|▍         | 38/800 [08:38<2:44:54, 12.99s/it]                                                  {'loss': 0.7899, 'learning_rate': 0.0004972216246861262, 'epoch': 6.33}
  5%|▍         | 38/800 [08:38<2:44:54, 12.99s/it]  5%|▍         | 39/800 [08:51<2:44:17, 12.95s/it]  5%|▌         | 40/800 [09:04<2:43:42, 12.92s/it]                                                  {'loss': 0.7459, 'learning_rate': 0.0004969220851487844, 'epoch': 6.67}
  5%|▌         | 40/800 [09:04<2:43:42, 12.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                  
                                     [A{'eval_loss': 0.4500020742416382, 'eval_runtime': 4.9812, 'eval_samples_per_second': 0.402, 'eval_steps_per_second': 0.201, 'epoch': 6.67}
  5%|▌         | 40/800 [09:09<2:43:42, 12.92s/it]
100%|██████████| 1/1 [00:00<00:00, 44.65it/s][A
                                             [A  5%|▌         | 41/800 [09:22<3:02:17, 14.41s/it]  5%|▌         | 42/800 [09:35<2:56:11, 13.95s/it]                                                  {'loss': 0.8234, 'learning_rate': 0.0004966073142941239, 'epoch': 7.0}
  5%|▌         | 42/800 [09:35<2:56:11, 13.95s/it]  5%|▌         | 43/800 [09:48<2:51:54, 13.63s/it]  6%|▌         | 44/800 [10:01<2:48:48, 13.40s/it]                                                  {'loss': 0.6633, 'learning_rate': 0.0004962773315386935, 'epoch': 7.33}
  6%|▌         | 44/800 [10:01<2:48:48, 13.40s/it]  6%|▌         | 45/800 [10:13<2:46:36, 13.24s/it]  6%|▌         | 46/800 [10:26<2:44:55, 13.12s/it]                                                  {'loss': 0.6701, 'learning_rate': 0.000495932157237384, 'epoch': 7.67}
  6%|▌         | 46/800 [10:26<2:44:55, 13.12s/it]  6%|▌         | 47/800 [10:39<2:43:41, 13.04s/it]  6%|▌         | 48/800 [10:52<2:42:46, 12.99s/it]                                                  {'loss': 0.6773, 'learning_rate': 0.0004955718126821722, 'epoch': 8.0}
  6%|▌         | 48/800 [10:52<2:42:46, 12.99s/it]  6%|▌         | 49/800 [11:05<2:42:07, 12.95s/it]  6%|▋         | 50/800 [11:18<2:41:33, 12.93s/it]                                                  {'loss': 0.67, 'learning_rate': 0.0004951963201008077, 'epoch': 8.33}
  6%|▋         | 50/800 [11:18<2:41:33, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                  
                                     [A{'eval_loss': 0.48435062170028687, 'eval_runtime': 4.9833, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 8.33}
  6%|▋         | 50/800 [11:23<2:41:33, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 49.68it/s][A
                                             [A  6%|▋         | 51/800 [11:36<2:59:54, 14.41s/it]  6%|▋         | 52/800 [11:48<2:53:52, 13.95s/it]                                                  {'loss': 0.5767, 'learning_rate': 0.0004948057026554415, 'epoch': 8.67}
  6%|▋         | 52/800 [11:48<2:53:52, 13.95s/it]  7%|▋         | 53/800 [12:01<2:49:38, 13.63s/it]  7%|▋         | 54/800 [12:14<2:46:34, 13.40s/it]                                                  {'loss': 0.4611, 'learning_rate': 0.0004943999844411977, 'epoch': 9.0}
  7%|▋         | 54/800 [12:14<2:46:34, 13.40s/it]  7%|▋         | 55/800 [12:27<2:44:24, 13.24s/it]  7%|▋         | 56/800 [12:40<2:42:47, 13.13s/it]                                                  {'loss': 0.471, 'learning_rate': 0.0004939791904846869, 'epoch': 9.33}
  7%|▋         | 56/800 [12:40<2:42:47, 13.13s/it]  7%|▋         | 57/800 [12:53<2:41:37, 13.05s/it]  7%|▋         | 58/800 [13:06<2:40:39, 12.99s/it]                                                  {'loss': 0.5263, 'learning_rate': 0.0004935433467424624, 'epoch': 9.67}
  7%|▋         | 58/800 [13:06<2:40:39, 12.99s/it]  7%|▋         | 59/800 [13:19<2:39:58, 12.95s/it]  8%|▊         | 60/800 [13:31<2:39:24, 12.93s/it]                                                  {'loss': 0.3568, 'learning_rate': 0.0004930924800994192, 'epoch': 10.0}
  8%|▊         | 60/800 [13:31<2:39:24, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                  
                                     [A{'eval_loss': 0.47663554549217224, 'eval_runtime': 4.9828, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 10.0}
  8%|▊         | 60/800 [13:36<2:39:24, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 44.31it/s][A
                                             [A  8%|▊         | 61/800 [13:49<2:57:31, 14.41s/it]  8%|▊         | 62/800 [14:02<2:51:33, 13.95s/it]                                                  {'loss': 0.3642, 'learning_rate': 0.0004926266183671356, 'epoch': 10.33}
  8%|▊         | 62/800 [14:02<2:51:33, 13.95s/it]  8%|▊         | 63/800 [14:15<2:47:24, 13.63s/it]  8%|▊         | 64/800 [14:28<2:44:21, 13.40s/it]                                                  {'loss': 0.379, 'learning_rate': 0.0004921457902821578, 'epoch': 10.67}
  8%|▊         | 64/800 [14:28<2:44:21, 13.40s/it]  8%|▊         | 65/800 [14:41<2:42:11, 13.24s/it]  8%|▊         | 66/800 [14:54<2:40:33, 13.12s/it]                                                  {'loss': 0.2928, 'learning_rate': 0.0004916500255042268, 'epoch': 11.0}
  8%|▊         | 66/800 [14:54<2:40:33, 13.12s/it]  8%|▊         | 67/800 [15:06<2:39:22, 13.05s/it]  8%|▊         | 68/800 [15:19<2:38:32, 12.99s/it]                                                  {'loss': 0.2901, 'learning_rate': 0.0004911393546144495, 'epoch': 11.33}
  8%|▊         | 68/800 [15:19<2:38:32, 12.99s/it]  9%|▊         | 69/800 [15:32<2:37:49, 12.95s/it]  9%|▉         | 70/800 [15:45<2:37:17, 12.93s/it]                                                  {'loss': 0.2495, 'learning_rate': 0.0004906138091134118, 'epoch': 11.67}
  9%|▉         | 70/800 [15:45<2:37:17, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                  
                                     [A{'eval_loss': 0.5417651534080505, 'eval_runtime': 4.9868, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 11.67}
  9%|▉         | 70/800 [15:50<2:37:17, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 44.71it/s][A
                                             [A  9%|▉         | 71/800 [16:03<2:55:08, 14.42s/it]  9%|▉         | 72/800 [16:16<2:49:14, 13.95s/it]                                                  {'loss': 0.2082, 'learning_rate': 0.0004900734214192358, 'epoch': 12.0}
  9%|▉         | 72/800 [16:16<2:49:14, 13.95s/it]  9%|▉         | 73/800 [16:29<2:45:07, 13.63s/it]  9%|▉         | 74/800 [16:42<2:42:06, 13.40s/it]                                                  {'loss': 0.207, 'learning_rate': 0.0004895182248655798, 'epoch': 12.33}
  9%|▉         | 74/800 [16:42<2:42:06, 13.40s/it]  9%|▉         | 75/800 [16:54<2:40:00, 13.24s/it] 10%|▉         | 76/800 [17:07<2:38:24, 13.13s/it]                                                  {'loss': 0.2083, 'learning_rate': 0.0004889482536995825, 'epoch': 12.67}
 10%|▉         | 76/800 [17:07<2:38:24, 13.13s/it] 10%|▉         | 77/800 [17:20<2:37:14, 13.05s/it] 10%|▉         | 78/800 [17:33<2:36:19, 12.99s/it]                                                  {'loss': 0.144, 'learning_rate': 0.0004883635430797502, 'epoch': 13.0}
 10%|▉         | 78/800 [17:33<2:36:19, 12.99s/it] 10%|▉         | 79/800 [17:46<2:35:41, 12.96s/it] 10%|█         | 80/800 [17:59<2:35:07, 12.93s/it]                                                  {'loss': 0.1334, 'learning_rate': 0.0004877641290737884, 'epoch': 13.33}
 10%|█         | 80/800 [17:59<2:35:07, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                  
                                     [A{'eval_loss': 0.5160428285598755, 'eval_runtime': 4.9856, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 13.33}
 10%|█         | 80/800 [18:04<2:35:07, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 64.21it/s][A
                                             [A 10%|█         | 81/800 [18:17<2:52:41, 14.41s/it] 10%|█         | 82/800 [18:29<2:46:52, 13.94s/it]                                                  {'loss': 0.1301, 'learning_rate': 0.0004871500486563761, 'epoch': 13.67}
 10%|█         | 82/800 [18:29<2:46:52, 13.94s/it] 10%|█         | 83/800 [18:42<2:42:46, 13.62s/it] 10%|█         | 84/800 [18:55<2:39:49, 13.39s/it]                                                  {'loss': 0.1102, 'learning_rate': 0.00048652133970688633, 'epoch': 14.0}
 10%|█         | 84/800 [18:55<2:39:49, 13.39s/it] 11%|█         | 85/800 [19:08<2:37:43, 13.24s/it] 11%|█         | 86/800 [19:21<2:36:10, 13.12s/it]                                                  {'loss': 0.0717, 'learning_rate': 0.0004858780410070484, 'epoch': 14.33}
 11%|█         | 86/800 [19:21<2:36:10, 13.12s/it] 11%|█         | 87/800 [19:34<2:34:58, 13.04s/it] 11%|█         | 88/800 [19:47<2:34:05, 12.99s/it]                                                  {'loss': 0.0862, 'learning_rate': 0.0004852201922385564, 'epoch': 14.67}
 11%|█         | 88/800 [19:47<2:34:05, 12.99s/it] 11%|█         | 89/800 [20:00<2:33:27, 12.95s/it] 11%|█▏        | 90/800 [20:12<2:32:55, 12.92s/it]                                                  {'loss': 0.0833, 'learning_rate': 0.0004845478339806211, 'epoch': 15.0}
 11%|█▏        | 90/800 [20:12<2:32:55, 12.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                  
                                     [A{'eval_loss': 0.5459213256835938, 'eval_runtime': 4.9819, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 15.0}
 11%|█▏        | 90/800 [20:17<2:32:55, 12.92s/it]
100%|██████████| 1/1 [00:00<00:00, 90.39it/s][A
                                             [A 11%|█▏        | 91/800 [20:30<2:50:14, 14.41s/it] 12%|█▏        | 92/800 [20:43<2:44:30, 13.94s/it]                                                  {'loss': 0.0564, 'learning_rate': 0.00048386100770746686, 'epoch': 15.33}
 12%|█▏        | 92/800 [20:43<2:44:30, 13.94s/it] 12%|█▏        | 93/800 [20:56<2:40:27, 13.62s/it] 12%|█▏        | 94/800 [21:09<2:37:36, 13.40s/it]                                                  {'loss': 0.0473, 'learning_rate': 0.0004831597557857735, 'epoch': 15.67}
 12%|█▏        | 94/800 [21:09<2:37:36, 13.40s/it] 12%|█▏        | 95/800 [21:22<2:35:33, 13.24s/it] 12%|█▏        | 96/800 [21:35<2:33:59, 13.12s/it]                                                  {'loss': 0.0581, 'learning_rate': 0.00048244412147206283, 'epoch': 16.0}
 12%|█▏        | 96/800 [21:35<2:33:59, 13.12s/it] 12%|█▏        | 97/800 [21:47<2:32:53, 13.05s/it] 12%|█▏        | 98/800 [22:00<2:32:01, 12.99s/it]                                                  {'loss': 0.0323, 'learning_rate': 0.0004817141489100302, 'epoch': 16.33}
 12%|█▏        | 98/800 [22:00<2:32:01, 12.99s/it] 12%|█▏        | 99/800 [22:13<2:31:23, 12.96s/it] 12%|█▎        | 100/800 [22:26<2:31:11, 12.96s/it]                                                   {'loss': 0.0384, 'learning_rate': 0.0004809698831278217, 'epoch': 16.67}
 12%|█▎        | 100/800 [22:26<2:31:11, 12.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.5741119980812073, 'eval_runtime': 4.9778, 'eval_samples_per_second': 0.402, 'eval_steps_per_second': 0.201, 'epoch': 16.67}
 12%|█▎        | 100/800 [22:31<2:31:11, 12.96s/it]
100%|██████████| 1/1 [00:00<00:00, 74.86it/s][A
                                             [A 13%|█▎        | 101/800 [22:44<2:48:07, 14.43s/it] 13%|█▎        | 102/800 [22:57<2:42:24, 13.96s/it]                                                   {'loss': 0.0386, 'learning_rate': 0.0004802113700352566, 'epoch': 17.0}
 13%|█▎        | 102/800 [22:57<2:42:24, 13.96s/it] 13%|█▎        | 103/800 [23:10<2:38:23, 13.63s/it] 13%|█▎        | 104/800 [23:23<2:35:26, 13.40s/it]                                                   {'loss': 0.027, 'learning_rate': 0.00047943865642099525, 'epoch': 17.33}
 13%|█▎        | 104/800 [23:23<2:35:26, 13.40s/it] 13%|█▎        | 105/800 [23:35<2:33:24, 13.24s/it] 13%|█▎        | 106/800 [23:48<2:31:52, 13.13s/it]                                                   {'loss': 0.0267, 'learning_rate': 0.0004786517899496534, 'epoch': 17.67}
 13%|█▎        | 106/800 [23:48<2:31:52, 13.13s/it] 13%|█▎        | 107/800 [24:01<2:30:48, 13.06s/it] 14%|█▎        | 108/800 [24:14<2:29:55, 13.00s/it]                                                   {'loss': 0.0293, 'learning_rate': 0.0004778508191588613, 'epoch': 18.0}
 14%|█▎        | 108/800 [24:14<2:29:55, 13.00s/it] 14%|█▎        | 109/800 [24:27<2:29:14, 12.96s/it] 14%|█▍        | 110/800 [24:40<2:28:41, 12.93s/it]                                                   {'loss': 0.0221, 'learning_rate': 0.00047703579345627036, 'epoch': 18.33}
 14%|█▍        | 110/800 [24:40<2:28:41, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.5786412358283997, 'eval_runtime': 4.9795, 'eval_samples_per_second': 0.402, 'eval_steps_per_second': 0.201, 'epoch': 18.33}
 14%|█▍        | 110/800 [24:45<2:28:41, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 79.44it/s][A
                                             [A 14%|█▍        | 111/800 [24:58<2:45:30, 14.41s/it] 14%|█▍        | 112/800 [25:11<2:40:03, 13.96s/it]                                                   {'loss': 0.0219, 'learning_rate': 0.0004762067631165049, 'epoch': 18.67}
 14%|█▍        | 112/800 [25:11<2:40:03, 13.96s/it] 14%|█▍        | 113/800 [25:23<2:36:03, 13.63s/it] 14%|█▍        | 114/800 [25:36<2:33:15, 13.40s/it]                                                   {'loss': 0.0205, 'learning_rate': 0.00047536377927806143, 'epoch': 19.0}
 14%|█▍        | 114/800 [25:36<2:33:15, 13.40s/it] 14%|█▍        | 115/800 [25:49<2:31:12, 13.24s/it] 14%|█▍        | 116/800 [26:02<2:29:41, 13.13s/it]                                                   {'loss': 0.0133, 'learning_rate': 0.0004745068939401539, 'epoch': 19.33}
 14%|█▍        | 116/800 [26:02<2:29:41, 13.13s/it] 15%|█▍        | 117/800 [26:15<2:28:33, 13.05s/it] 15%|█▍        | 118/800 [26:28<2:27:45, 13.00s/it]                                                   {'loss': 0.0163, 'learning_rate': 0.00047363615995950624, 'epoch': 19.67}
 15%|█▍        | 118/800 [26:28<2:27:45, 13.00s/it] 15%|█▍        | 119/800 [26:41<2:27:05, 12.96s/it] 15%|█▌        | 120/800 [26:54<2:26:31, 12.93s/it]                                                   {'loss': 0.0161, 'learning_rate': 0.00047275163104709196, 'epoch': 20.0}
 15%|█▌        | 120/800 [26:54<2:26:31, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.5886490345001221, 'eval_runtime': 4.984, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 20.0}
 15%|█▌        | 120/800 [26:59<2:26:31, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 61.91it/s][A
                                             [A 15%|█▌        | 121/800 [27:11<2:43:07, 14.41s/it] 15%|█▌        | 122/800 [27:24<2:37:39, 13.95s/it]                                                   {'loss': 0.0108, 'learning_rate': 0.00047185336176482084, 'epoch': 20.33}
 15%|█▌        | 122/800 [27:24<2:37:39, 13.95s/it] 15%|█▌        | 123/800 [27:37<2:33:45, 13.63s/it] 16%|█▌        | 124/800 [27:50<2:30:57, 13.40s/it]                                                   {'loss': 0.0114, 'learning_rate': 0.0004709414075221734, 'epoch': 20.67}
 16%|█▌        | 124/800 [27:50<2:30:57, 13.40s/it] 16%|█▌        | 125/800 [28:03<2:28:59, 13.24s/it] 16%|█▌        | 126/800 [28:16<2:27:32, 13.13s/it]                                                   {'loss': 0.0156, 'learning_rate': 0.000470015824572783, 'epoch': 21.0}
 16%|█▌        | 126/800 [28:16<2:27:32, 13.13s/it] 16%|█▌        | 127/800 [28:29<2:26:27, 13.06s/it] 16%|█▌        | 128/800 [28:42<2:25:34, 13.00s/it]                                                   {'loss': 0.0105, 'learning_rate': 0.0004690766700109659, 'epoch': 21.33}
 16%|█▌        | 128/800 [28:42<2:25:34, 13.00s/it] 16%|█▌        | 129/800 [28:54<2:24:57, 12.96s/it] 16%|█▋        | 130/800 [29:07<2:24:25, 12.93s/it]                                                   {'loss': 0.0098, 'learning_rate': 0.0004681240017681993, 'epoch': 21.67}
 16%|█▋        | 130/800 [29:07<2:24:25, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.6037105917930603, 'eval_runtime': 4.9852, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 21.67}
 16%|█▋        | 130/800 [29:12<2:24:25, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 72.42it/s][A
                                             [A 16%|█▋        | 131/800 [29:25<2:40:44, 14.42s/it] 16%|█▋        | 132/800 [29:38<2:35:17, 13.95s/it]                                                   {'loss': 0.0086, 'learning_rate': 0.00046715787860954785, 'epoch': 22.0}
 16%|█▋        | 132/800 [29:38<2:35:17, 13.95s/it] 17%|█▋        | 133/800 [29:51<2:31:30, 13.63s/it] 17%|█▋        | 134/800 [30:04<2:28:44, 13.40s/it]                                                   {'loss': 0.007, 'learning_rate': 0.0004661783601300388, 'epoch': 22.33}
 17%|█▋        | 134/800 [30:04<2:28:44, 13.40s/it] 17%|█▋        | 135/800 [30:17<2:26:46, 13.24s/it] 17%|█▋        | 136/800 [30:29<2:25:19, 13.13s/it]                                                   {'loss': 0.0094, 'learning_rate': 0.0004651855067509859, 'epoch': 22.67}
 17%|█▋        | 136/800 [30:30<2:25:19, 13.13s/it] 17%|█▋        | 137/800 [30:42<2:24:14, 13.05s/it] 17%|█▋        | 138/800 [30:55<2:23:22, 12.99s/it]                                                   {'loss': 0.0066, 'learning_rate': 0.00046417937971626245, 'epoch': 23.0}
 17%|█▋        | 138/800 [30:55<2:23:22, 12.99s/it] 17%|█▋        | 139/800 [31:08<2:22:46, 12.96s/it] 18%|█▊        | 140/800 [31:21<2:22:13, 12.93s/it]                                                   {'loss': 0.0083, 'learning_rate': 0.00046316004108852305, 'epoch': 23.33}
 18%|█▊        | 140/800 [31:21<2:22:13, 12.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.6179966330528259, 'eval_runtime': 4.9877, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.2, 'epoch': 23.33}
 18%|█▊        | 140/800 [31:26<2:22:13, 12.93s/it]
100%|██████████| 1/1 [00:00<00:00, 75.17it/s][A
                                             [A 18%|█▊        | 141/800 [31:39<2:38:19, 14.42s/it] 18%|█▊        | 142/800 [31:52<2:32:59, 13.95s/it]                                                   {'loss': 0.0041, 'learning_rate': 0.00046212755374537594, 'epoch': 23.67}
 18%|█▊        | 142/800 [31:52<2:32:59, 13.95s/it] 18%|█▊        | 143/800 [32:05<2:29:14, 13.63s/it] 18%|█▊        | 144/800 [32:17<2:26:32, 13.40s/it]                                                   {'loss': 0.0066, 'learning_rate': 0.00046108198137550377, 'epoch': 24.0}
 18%|█▊        | 144/800 [32:17<2:26:32, 13.40s/it] 18%|█▊        | 145/800 [32:30<2:24:34, 13.24s/it] 18%|█▊        | 146/800 [32:43<2:23:08, 13.13s/it]                                                   {'loss': 0.0046, 'learning_rate': 0.00046002338847473545, 'epoch': 24.33}
 18%|█▊        | 146/800 [32:43<2:23:08, 13.13s/it] 18%|█▊        | 147/800 [32:56<2:22:09, 13.06s/it] 18%|█▊        | 148/800 [33:09<2:21:18, 13.00s/it]                                                   {'loss': 0.0056, 'learning_rate': 0.0004589518403420676, 'epoch': 24.67}
 18%|█▊        | 148/800 [33:09<2:21:18, 13.00s/it] 19%|█▊        | 149/800 [33:22<2:20:42, 12.97s/it] 19%|█▉        | 150/800 [33:35<2:20:10, 12.94s/it]                                                   {'loss': 0.0049, 'learning_rate': 0.00045786740307563633, 'epoch': 25.0}
 19%|█▉        | 150/800 [33:35<2:20:10, 12.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.6059155464172363, 'eval_runtime': 4.9901, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.2, 'epoch': 25.0}
 19%|█▉        | 150/800 [33:40<2:20:10, 12.94s/it]
100%|██████████| 1/1 [00:00<00:00, 42.03it/s][A
                                             [A 19%|█▉        | 151/800 [33:53<2:36:02, 14.43s/it] 19%|█▉        | 152/800 [34:05<2:30:44, 13.96s/it]                                                   {'loss': 0.0049, 'learning_rate': 0.00045677014356864043, 'epoch': 25.33}
 19%|█▉        | 152/800 [34:06<2:30:44, 13.96s/it] 19%|█▉        | 153/800 [34:18<2:26:58, 13.63s/it] 19%|█▉        | 154/800 [34:31<2:24:17, 13.40s/it]                                                   {'loss': 0.0051, 'learning_rate': 0.00045566012950521497, 'epoch': 25.67}
 19%|█▉        | 154/800 [34:31<2:24:17, 13.40s/it] 19%|█▉        | 155/800 [34:44<2:22:22, 13.24s/it] 20%|█▉        | 156/800 [34:57<2:20:56, 13.13s/it]                                                   {'loss': 0.003, 'learning_rate': 0.0004545374293562559, 'epoch': 26.0}
 20%|█▉        | 156/800 [34:57<2:20:56, 13.13s/it] 20%|█▉        | 157/800 [35:10<2:19:55, 13.06s/it] 20%|█▉        | 158/800 [35:23<2:19:08, 13.00s/it]                                                   {'loss': 0.0025, 'learning_rate': 0.0004534021123751968, 'epoch': 26.33}
 20%|█▉        | 158/800 [35:23<2:19:08, 13.00s/it] 20%|█▉        | 159/800 [35:36<2:18:31, 12.97s/it] 20%|██        | 160/800 [35:48<2:18:02, 12.94s/it]                                                   {'loss': 0.0049, 'learning_rate': 0.0004522542485937369, 'epoch': 26.67}
 20%|██        | 160/800 [35:49<2:18:02, 12.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.6140143871307373, 'eval_runtime': 4.9859, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 26.67}
 20%|██        | 160/800 [35:54<2:18:02, 12.94s/it]
100%|██████████| 1/1 [00:00<00:00, 63.17it/s][A
                                             [A 20%|██        | 161/800 [36:06<2:33:39, 14.43s/it] 20%|██        | 162/800 [36:19<2:28:26, 13.96s/it]                                                   {'loss': 0.0047, 'learning_rate': 0.0004510939088175211, 'epoch': 27.0}
 20%|██        | 162/800 [36:19<2:28:26, 13.96s/it] 20%|██        | 163/800 [36:32<2:24:49, 13.64s/it] 20%|██        | 164/800 [36:45<2:22:09, 13.41s/it]                                                   {'loss': 0.0028, 'learning_rate': 0.0004499211646217727, 'epoch': 27.33}
 20%|██        | 164/800 [36:45<2:22:09, 13.41s/it] 21%|██        | 165/800 [36:58<2:20:15, 13.25s/it] 21%|██        | 166/800 [37:11<2:18:51, 13.14s/it]                                                   {'loss': 0.004, 'learning_rate': 0.00044873608834687754, 'epoch': 27.67}
 21%|██        | 166/800 [37:11<2:18:51, 13.14s/it] 21%|██        | 167/800 [37:24<2:17:47, 13.06s/it] 21%|██        | 168/800 [37:37<2:16:58, 13.00s/it]                                                   {'loss': 0.0032, 'learning_rate': 0.0004475387530939226, 'epoch': 28.0}
 21%|██        | 168/800 [37:37<2:16:58, 13.00s/it] 21%|██        | 169/800 [37:49<2:16:24, 12.97s/it] 21%|██▏       | 170/800 [38:02<2:15:53, 12.94s/it]                                                   {'loss': 0.0023, 'learning_rate': 0.0004463292327201862, 'epoch': 28.33}
 21%|██▏       | 170/800 [38:02<2:15:53, 12.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.6145886182785034, 'eval_runtime': 4.9772, 'eval_samples_per_second': 0.402, 'eval_steps_per_second': 0.201, 'epoch': 28.33}
 21%|██▏       | 170/800 [38:07<2:15:53, 12.94s/it]
100%|██████████| 1/1 [00:00<00:00, 46.76it/s][A
                                             [A 21%|██▏       | 171/800 [38:20<2:31:14, 14.43s/it] 22%|██▏       | 172/800 [38:33<2:26:06, 13.96s/it]                                                   {'loss': 0.0033, 'learning_rate': 0.0004451076018345824, 'epoch': 28.67}
 22%|██▏       | 172/800 [38:33<2:26:06, 13.96s/it] 22%|██▏       | 173/800 [38:46<2:22:29, 13.63s/it] 22%|██▏       | 174/800 [38:59<2:19:49, 13.40s/it]                                                   {'loss': 0.0032, 'learning_rate': 0.0004438739357930586, 'epoch': 29.0}
 22%|██▏       | 174/800 [38:59<2:19:49, 13.40s/it] 22%|██▏       | 175/800 [39:12<2:17:58, 13.24s/it] 22%|██▏       | 176/800 [39:25<2:16:33, 13.13s/it]                                                   {'loss': 0.0028, 'learning_rate': 0.0004426283106939473, 'epoch': 29.33}
 22%|██▏       | 176/800 [39:25<2:16:33, 13.13s/it] 22%|██▏       | 177/800 [39:37<2:15:31, 13.05s/it] 22%|██▏       | 178/800 [39:50<2:14:45, 13.00s/it]                                                   {'loss': 0.002, 'learning_rate': 0.00044137080337327205, 'epoch': 29.67}
 22%|██▏       | 178/800 [39:50<2:14:45, 13.00s/it] 22%|██▏       | 179/800 [40:03<2:14:13, 12.97s/it] 22%|██▎       | 180/800 [40:16<2:13:40, 12.94s/it]                                                   {'loss': 0.003, 'learning_rate': 0.0004401014914000078, 'epoch': 30.0}
 22%|██▎       | 180/800 [40:16<2:13:40, 12.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.6277486085891724, 'eval_runtime': 4.9808, 'eval_samples_per_second': 0.402, 'eval_steps_per_second': 0.201, 'epoch': 30.0}
 22%|██▎       | 180/800 [40:21<2:13:40, 12.94s/it]
100%|██████████| 1/1 [00:00<00:00, 49.74it/s][A
                                             [A 23%|██▎       | 181/800 [40:34<2:28:48, 14.42s/it] 23%|██▎       | 182/800 [40:47<2:23:47, 13.96s/it]                                                   {'loss': 0.0021, 'learning_rate': 0.0004388204530712959, 'epoch': 30.33}
 23%|██▎       | 182/800 [40:47<2:23:47, 13.96s/it] 23%|██▎       | 183/800 [41:00<2:20:36, 13.67s/it] 23%|██▎       | 184/800 [41:13<2:17:55, 13.43s/it]                                                   {'loss': 0.0016, 'learning_rate': 0.0004375277674076149, 'epoch': 30.67}
 23%|██▎       | 184/800 [41:13<2:17:55, 13.43s/it] 23%|██▎       | 185/800 [41:26<2:16:01, 13.27s/it] 23%|██▎       | 186/800 [41:38<2:14:36, 13.15s/it]                                                   {'loss': 0.0023, 'learning_rate': 0.0004362235141479055, 'epoch': 31.0}
 23%|██▎       | 186/800 [41:38<2:14:36, 13.15s/it] 23%|██▎       | 187/800 [41:51<2:13:35, 13.08s/it] 24%|██▎       | 188/800 [42:04<2:12:44, 13.01s/it]                                                   {'loss': 0.0016, 'learning_rate': 0.00043490777374465244, 'epoch': 31.33}
 24%|██▎       | 188/800 [42:04<2:12:44, 13.01s/it] 24%|██▎       | 189/800 [42:17<2:12:08, 12.98s/it] 24%|██▍       | 190/800 [42:30<2:11:34, 12.94s/it]                                                   {'loss': 0.0013, 'learning_rate': 0.0004335806273589214, 'epoch': 31.67}
 24%|██▍       | 190/800 [42:30<2:11:34, 12.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.6193407773971558, 'eval_runtime': 4.985, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 31.67}
 24%|██▍       | 190/800 [42:35<2:11:34, 12.94s/it]
100%|██████████| 1/1 [00:00<00:00, 73.65it/s][A
                                             [A 24%|██▍       | 191/800 [42:48<2:26:23, 14.42s/it] 24%|██▍       | 192/800 [43:01<2:21:27, 13.96s/it]                                                   {'loss': 0.0021, 'learning_rate': 0.00043224215685535287, 'epoch': 32.0}
 24%|██▍       | 192/800 [43:01<2:21:27, 13.96s/it] 24%|██▍       | 193/800 [43:14<2:17:52, 13.63s/it] 24%|██▍       | 194/800 [43:26<2:15:21, 13.40s/it]                                                   {'loss': 0.0015, 'learning_rate': 0.00043089244479711233, 'epoch': 32.33}
 24%|██▍       | 194/800 [43:26<2:15:21, 13.40s/it] 24%|██▍       | 195/800 [43:39<2:13:36, 13.25s/it] 24%|██▍       | 196/800 [43:52<2:12:15, 13.14s/it]                                                   {'loss': 0.0015, 'learning_rate': 0.0004295315744407972, 'epoch': 32.67}
 24%|██▍       | 196/800 [43:52<2:12:15, 13.14s/it] 25%|██▍       | 197/800 [44:05<2:11:16, 13.06s/it] 25%|██▍       | 198/800 [44:18<2:10:31, 13.01s/it]                                                   {'loss': 0.0013, 'learning_rate': 0.00042815962973130134, 'epoch': 33.0}
 25%|██▍       | 198/800 [44:18<2:10:31, 13.01s/it] 25%|██▍       | 199/800 [44:31<2:09:56, 12.97s/it] 25%|██▌       | 200/800 [44:44<2:09:24, 12.94s/it]                                                   {'loss': 0.0014, 'learning_rate': 0.00042677669529663686, 'epoch': 33.33}
 25%|██▌       | 200/800 [44:44<2:09:24, 12.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.6295480728149414, 'eval_runtime': 4.9871, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 33.33}
 25%|██▌       | 200/800 [44:49<2:09:24, 12.94s/it]
100%|██████████| 1/1 [00:00<00:00, 90.27it/s][A
                                             [A 25%|██▌       | 201/800 [45:02<2:23:59, 14.42s/it] 25%|██▌       | 202/800 [45:15<2:19:06, 13.96s/it]                                                   {'loss': 0.0011, 'learning_rate': 0.000425382856442714, 'epoch': 33.67}
 25%|██▌       | 202/800 [45:15<2:19:06, 13.96s/it] 25%|██▌       | 203/800 [45:27<2:15:38, 13.63s/it] 26%|██▌       | 204/800 [45:40<2:13:09, 13.40s/it]                                                   {'loss': 0.0011, 'learning_rate': 0.00042397819914807855, 'epoch': 34.0}
 26%|██▌       | 204/800 [45:40<2:13:09, 13.40s/it] 26%|██▌       | 205/800 [45:53<2:11:24, 13.25s/it] 26%|██▌       | 206/800 [46:06<2:10:05, 13.14s/it]                                                   {'loss': 0.0012, 'learning_rate': 0.0004225628100586093, 'epoch': 34.33}
 26%|██▌       | 206/800 [46:06<2:10:05, 13.14s/it] 26%|██▌       | 207/800 [46:19<2:09:04, 13.06s/it] 26%|██▌       | 208/800 [46:32<2:08:19, 13.01s/it]                                                   {'loss': 0.0008, 'learning_rate': 0.0004211367764821722, 'epoch': 34.67}
 26%|██▌       | 208/800 [46:32<2:08:19, 13.01s/it] 26%|██▌       | 209/800 [46:45<2:07:44, 12.97s/it] 26%|██▋       | 210/800 [46:58<2:07:12, 12.94s/it]                                                   {'loss': 0.0009, 'learning_rate': 0.00041970018638323546, 'epoch': 35.0}
 26%|██▋       | 210/800 [46:58<2:07:12, 12.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                   
                                     [A{'eval_loss': 0.6303739547729492, 'eval_runtime': 4.9818, 'eval_samples_per_second': 0.401, 'eval_steps_per_second': 0.201, 'epoch': 35.0}
 26%|██▋       | 210/800 [47:03<2:07:12, 12.94s/it]
100%|██████████| 1/1 [00:00<00:00, 49.93it/s][A
                                             [A 26%|██▋       | 211/800 [47:15<2:21:33, 14.42s/it]srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 190846.0 ON compute34 CANCELLED AT 2024-03-04T01:21:56 ***

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 0 PID 185082 RUNNING AT compute34
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 1 PID 185083 RUNNING AT compute34
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 2 PID 185084 RUNNING AT compute34
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 3 PID 185085 RUNNING AT compute34
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 4 PID 185086 RUNNING AT compute34
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 5 PID 185087 RUNNING AT compute34
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 6 PID 185088 RUNNING AT compute34
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================

===================================================================================
=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES
=   RANK 7 PID 185089 RUNNING AT compute34
=   KILLED BY SIGNAL: 15 (Terminated)
===================================================================================
